/*! \page sparse Using STRUMPACK Sparse

This section gives an overview on the basic usage of the sparse solvers in STRUMPACK. Many STRUMPACK options can be set from the command line. Running with \--help or -h, will give you a list of supported run- time options. 

An example Makefile is available in the examples/ directory. This Makefile is generated by the __cmake__ command, see Section 2.

The STRUMPACK package is written in C++, and offers a simple C++ interface. See Section 8 if you prefer a C interface. STRUMPACK-sparse has three different solver classes, all interaction happens through objects of these classes: 

- __StrumpackSparseSolver<scalar,integer=int>__    
This class represents the sparse solver for a single computational node, optionally using OpenMP parallelism. Use this if you are running the code sequentially, on a (multicore) laptop or desktop or on a single node of a larger cluster. This class is defined in StrumpackSparseSolver.hpp, so include this header if you intend to use it.
- __StrumpackSparseSolverMPI<scalar,integer=int>__    
This solver has (mostly) the same interface as StrumpackSparseSolver<scalar,integer> but the numerical factorization and multifrontal solve phases run in parallel using MPI and ScaLAPACK. However, the inputs (sparse matrix, right-hand side vector) need to be available completely on every MPI process. The reordering phase uses Metis or Scotch (not ParMetis or PTScotch) and the symbolic factorization is threaded, but not distributed. The (multifrontal) solve is done in parallel, but the right-hand side vectors need to be available completely on every processor. Make sure to call MPI_Init[_thread] before instantiating an object of this class and include the header file StrumpackSparseSolverMPI.hpp. We do not recommend this solver, instead, use StrumpackSparseSolverMPIDist whenever possible.
- __StrumpackSparseSolverMPIDist<scalar,integer=int>__    
This solver is fully distributed. The numerical factorization and solve as well as the symbolic factor- ization are distributed. The input is now a block-row distributed sparse matrix and a correspondingly distributed right-hand side. For matrix reordering, ParMetis or PT-Scotch are used. Include the header file StrumpackSparseSolverMPIDist.hpp and call MPI_Init[_thread].

The three solver classes StrumpackSparseSolver, StrumpackSparseSolverMPI and StrumpackSparseSolverMPIDist depend on two template parameters <scalar,integer>: the type of a scalar and an integer type. The scalar type can be float, double, std::complex<float> or std::complex<double>. It is recommended to first try to simply use the default integer=int type, unless you run into 32 bit integer overflow problems. In that case one can switch to for instance int64_t (a signed integer type).

____

- \subpage StrumpackSparseSolver_Example 
- \subpage StrumpackSparseSolverMPI_Example
- \subpage StrumpackSparseSolverMPIDist_Example
- \subpage Initializing_the_Solver_Object
- \subpage Sparse_Matrix_Format
- \subpage Setting_and_Parsing_Options
- \subpage reordering
- \subpage factorization
- \subpage solve
- \subpage sparse_solver_options

*/

/*! \page StrumpackSparseSolver_Example StrumpackSparseSolver Example

The following shows the typical way to use a (sequential or multithreaded) STRUMPACK sparse solver: 

<img src="/Users/lucyguo/Desktop/LBL1/STRUMPACK/doc/doxygen/images/SSSEx.png" width = 100%>

Figure 1: Illustration of a small 5 × 5 sparse matrix with 11 nonzeros and its Compressed Sparse Row (CSR) or Yale format representation. We always use 0-based indexing! Let \f$N\f$ = 5 denote the number of rows. The row_ptr array has \f$N\f$ +1 elements, with element \f$i\f$ denoting the start of row \f$i\f$ in the col_ind and values arrays. Element row_ptr[N] = nnz, i.e., the total number of nonzero elements in the matrix. The values array holds the actual matrix values, ordered by row. The corresponding elements in col_ind give the column indices for each nonzero. There can be explicit zero elements in the matrix. The nonzero values and corresponding column indices need not be sorted by column (within a row).

\code {.cpp}
 #include "StrumpackSparseSolver.hpp"    
 using namespace strumpack;      // all strumpack code is in the strumpack namespace,       
 
 int main(int argc, char* argv[]) {    
    int N = ...;                      // construct an NxN CSR matrix with nnz nonzeros    
    int* row_ptr = ...;               // N+1 integers    
    int* col_ind = ...;               // nnz integers    
    double* val = ...;                // nnz scalars    
    double* x = new double[N];        // will hold the solution vector    
    double* b = ...;                  // set a right-hand side b    
    
    StrumpackSparseSolver<double> sp;                   // create solver object    
    sp.options().set_rel_tol(1e-10);                    // set options    
    sp.options().set_gmres_restart(10); &               // ...    
    sp.options().enable_HSS();                          // enable HSS compression, see section 5    
    sp.options().set_from_command_line(argc, argv);     // parse command line options    
    sp.set_csr_matrix(N, row_ptr, col_ind, val);        // set the matrix (copy)    
    sp.reorder();                                       // reorder matrix    
    sp.factor();                                        // numerical factorization    
    sp.solve(b, x);                                     // solve Ax=b    
    ... // check residual/error and cleanup    
 }
\endcode

The main steps are: create solver object, set options (parse options from the command line), set matrix, reorder, factor and finally solve. The matrix should be in the Compressed Sparse Row (CSR) format, also called Yale format, with 0 based indices. Figure 1 illustrates the CSR format. In the basic scenario, it is not necessary to explicitly call reorder and factor, since trying to solve with a StrumpackSparseSolver object that is not factored yet, will internally call the factor routine, which will call reorder if necessary.
   
The above code should be linked with -lstrumpack and with the Metis, ParMetis, Scotch, PT-Scotch, BLAS, LAPACK, and ScaLAPACK libraries.

*/

/*! \page StrumpackSparseSolverMPI_Example StrumpackSparseSolverMPI Example

Usage of the StrumpackSparseSolverMPI<scalar,integer=int> solver is very similar:    

\code {.cpp}
 #include "StrumpackSparseSolverMPI.hpp"    
 using namespace strumpack;    
     
 int main(int argc, char* argv[ ])&nbsp;&nbsp; {    
     int thread_level, rank;    
     // StrumpackSparseSolverMPI uses OpenMP so we should ask for MPI_THREAD_FUNNELED at least    
     MPI_Init_thread(&argc, &argv, MPI_THREAD_FUNNELED, &thread_level);     
     MPI_Comm_rank(MPI_COMM_WORLD, &rank);    
     if (thread_level != MPI_THREAD_FUNNELED && rank == 0)
        std::cout << "MPI␣implementation␣does␣not␣support␣MPI_THREAD_FUNNELED" << std::endl;    
     
     {    
         // define the same CSR matrix as for StrumpackSparseSolver    
         int N=...;                    // construct an NxN CSR matrix with nnz nonzeros    
         int* row_ptr = ...;           // N+1 integers    
         int* col_ind = ...;           // nnz integers    
         double* val = ...;            // nnz scalars    
         // allocate entire solution and right-hand side vectors on each MPI process    
         double* x = new double[N];    // will hold the solution vector    
         double* b = ...;              // set a right-hand side b    
     
         // construct solver and specify the MPI communicator    
         StrumpackSparseSolverMPI<double> sp(MPI_COMM_WORLD);     
         sp.options().set_matching(MatchingJob::NONE);     
         sp.options().set_from_command_line(argc, argv);     
         sp.set_csr_matrix(N, row_ptr, col_ind, val); sp.solve(b, x);    
         ... // check residual/error, cleanup    
    }    
    Cblacs_exit(1);    
    MPI_Finalize();    
 }
\endcode

The only difference here is the use of StrumpackSparseSolverMPI instead of StrumpackSparseSolver and the calls to MPI_Init_thread, Cblacs_exit and MPI_Finalize.

*/

/*! \page StrumpackSparseSolverMPIDist_Example StrumpackSparseSolverMPIDist Example

Finally, we illustrate the usage of StrumpackSparseSolverMPIDist<scalar,integer=int> solver. This interface takes a block-row distributed compressed sparse row matrix as input. This matrix format is illustrated in Figure 2.

\code {.cpp}
 #include "StrumpackSparseSolverMPI.hpp"    
 using namespace strumpack;    
     
 int main(int argc, char* argv[ ]) &nbsp;&nbsp;{    
     int thread_level, rank, P;    
     MPI_Init_thread(&argc, &argv, MPI_THREAD_FUNNELED, &thread_level);     
     MPI_Comm_rank(MPI_COMM_WORLD, &rank);     
     MPI_Comm_size(MPI_COMM_WORLD, &P);    
     {    
         // define a block-row distributed CSR matrix    
         int* dist = new int[P];    
         // set dist such that processor p owns rows [dist[p], dist[p+1]) of the sparse matrix     
         for (int p=0; p<P; p++) dist[p] = .. ;    
         // local_n is the number of rows of the input matrix assigned to me
\endcode

<img src="/Users/lucyguo/Desktop/LBL1/STRUMPACK/doc/doxygen/images/SSSMPIDistEx.png" width = 100%>

Figure 2: Illustration of a small 5×5 sparse matrix with 11 nonzeros and its block-row distributed compressed sparse row representation. We always use 0-based indexing! Process P0 owns row 0, process P1 has rows 1 and 2 and process P2 has rows 3 and 4. This distribution of rows over the processes is represented by the dist array. Process p owns rows [dist[p],dist[p+1]). If N = 5 is the number of rows in the entire matrix and P is the total number of processes, then dist[P]=N. The (same) dist array is stored on every process. Each process holds a CSR representation of only its local rows of the matrix, see Figure 1.

\code {.cpp}
         int local_n   = dist[rank+1] - dist[rank];    
         int* row_ptr  = new int[local_n+1];    
         .. // set the sparse matrix row pointers in row_ptr    
         int local_nnz = row_ptr[local_n+1] - row_ptr[0];    
         int* col_ind  = new int[local_nnz];    
         .. // set the sparse matrix column indices in col_ind    
         double* val   = new double[local_nnz];    
         .. // set the matrix nonzero value in val    
         double* x = new double[local_n];             // local part of solution    
         double* b = new double[local_n];             // local part of rhs    
         for (int i=0; i<local_n; i++) b[i] = ..;     // set the rhs    
     
         StrumpackSparseSolverMPIDist<double> sp(MPI_COMM_WORLD);     
         sp.options().set_reordering_method(ReorderingStrategy::PARMETIS);     
         sp.options().set_from_command_line(argc, argv);     
         sp.set_distributed_csr_matrix(local_n, row_ptr, col_ind, val, dist);    
         sp.solve(b, x);    
         ... // check residual/error, cleanup    
     }    
     Cblacs_exit(1);    
     MPI_Finalize();    
}
\endcode

*/

/*! \page Initializing_the_Solver_Object Initializing the Solver Object

Let

\code {.cpp}
typedef strumpack::StrumpackSparseSolver<scalar,integer> Sp;    
typedef strumpack::StrumpackSparseSolverMPI<scalar,integer> SpMPI;    
typedef strumpack::StrumpackSparseSolverMPIDist<scalar,integer> SpMPIDist;
\endcode
   
Each of the solver classes has two constructors:

\code {.cpp}
Sp::StrumpackSparseSolver(bool verbose=true, bool root=true);    
Sp::StrumpackSparseSolver(int argc, char* argv[], bool verbose=true, bool root=true); 
\endcode

\code {.cpp}
SpMPI::StrumpackSparseSolverMPIDist(MPI_Comm comm, bool verbose=true);    
SpMPI::StrumpackSparseSolverMPIDist(MPI_Comm comm, int argc, char* argv[], bool verbose=true); 
\endcode

\code {.cpp}
SpMPIDist::StrumpackSparseSolverMPIDist(MPI_Comm comm, bool verbose=true);    
SpMPIDist::StrumpackSparseSolverMPIDist(MPI_Comm comm, int argc, char* argv[], bool verbose=true); 
\endcode
 
where argc and argv contain the command line options and the verbose option can be set to false to suppress output of the solver. Note that since SpMPIDist is a subclass of SpMPI, which is a subclass of Sp, all public members of Sp are also members of SpMPI and SpMPIDist. The public interface to the SpMPI class is exactly the same as that for the Sp class.

*/

/*! \page Sparse_Matrix_Format Sparse Matrix Format

The sparse matrix should be specified in compressed sparse row format [8]:

\code {.cpp}
void Sp :: set_csr_matrix(int N, int* row_ptr, int* col_ind, scalar* values, bool symmetric_pattern=false);
\endcode

Internally, the matrix is copied, so it will not be modified. Previous versions of STRUMPACK also supported the CSC format, but this is now deprecated. If the sparsity pattern of the matrix is symmetric (the values do not have to be symmetric), then you can set symmetric_pattern=true. This saves some work in the setup phase of the solver.

For the SpMPIDist solver the input is a block-row distributed compressed sparse row matrix (as illustrated in the example above):

\code {.cpp}
void SpMPIDist :: set_distributed_csr_matrix (integer local_rows, integer* row_ptr, integer* col_ind, scalar* values, integer* dist, bool symmetric_pattern=false);
\endcode

Alternatively, you can also specify a sequential CSR matrix to the SpMPIDist solver:

\code {.cpp}
void SpMPIDist :: set_csr_matrix (integer N, integer* row_ptr, integer* col_ind, scalar* values, bool symmetric_pattern=false);
\endcode

For this routine, the matrix only needs to be specified completely on the root process. Other processes can pass NULL for the arrays.

*/

/*! \page Setting_and_Parsing_Options Setting and Parsing Options

The solver class has an object of type SPOptions<scalar>, which can be accessed through:

\code {.cpp} SPOptions<scalar>& Sp::options(); \endcode

The strumpack::SPOptions class is defined in StrumpackOptions.hpp. The complete public interface for the SPOptions<scalar> class is given in Section 4.10.1. The following subsections describe some of the options available from SPOptions<scalar> in more detail.

*/

/*! \page reordering Reordering

There are three types of matrix reordering: for numerical stability, to reduce fill-in and to reduce the HSS- ranks. These reorderings are all performed when calling

\code {.cpp} ReturnCode Sp :: reorder( ); \endcode

The return value is of type ReturnCode (defined in strumpack_parameters.hpp) and can be

\code {.cpp}
enum class ReturnCode {    
   SUCCESS,           /*!< Operation completed successfully. */     
   MATRIX_NOT_SET,    /*!< The input matrix was not set. */     
   REORDERING_ERROR   /*!< The matrix reordering failed. */    
};
\endcode

____
- \subpage Reordering_for_Numerical_Stability
- \subpage Nested_Dissection_Recording

*/

/*! \page Reordering_for_Numerical_Stability Reordering for Numerical Stability

The reordering for numerical stability is performed using MC64 or Combinatorial BLAS. For many matrices, this reordering is not necessary and can safely be disabled! MC64 supports 5 different modes and there is one option to select the Combinatorial BLAS code:

\code {.cpp}
enum class MatchingJob {     
    NONE,                           /*!< Don’t do anything */      
    MAX_CARDINALITY,                /*!< Maximum cardinality */      
    MAX_SMALLEST_DIAGONAL,          /*!< Maximum smallest diagonal value */      
    MAX_SMALLEST_DIAGONAL_2,        /*!< Same as MAX_SMALLEST_DIAGONAL, different algorithm */   
    MAX_DIAGONAL_SUM,               /*!< Maximum sum of diagonal values */      
    MAX_DIAGONAL_PRODUCT_SCALING,   /*!< Maximum product of diagonal values and row and column scaling */      
    COMBBLAS                        /*!< Use AWPM from Combinatorial BLAS */     
};
\endcode

which can be selected via

\code {.cpp}
void SPOptions::set_matching(MatchingJob job);    
MatchingJob SPOptions::matching() const;
\endcode

where matching() queries the currently selected strategy (the default is MAX_DIAGONAL_PRODUCT_SCALING maximum product of diagonal values plus row and column scaling). The command line option

\code {.cpp}--sp_matching [0-6] \endcode

can also be used, where the integers are defined as:
- 0: no reordering for stability, this disables MC64/matching
- 1: MC64(1): currently not supported
- 2: MC64(2): maximize the smallest diagonal value
- 3: MC64(3): maximize the smallest diagonal value, different strategy
- 4: MC64(4): maximize sum of diagonal values
- 5: MC64(5): maximize product of diagonal values and apply row and column scaling 6: Combinatorial BLAS: approximate weight perfect matching

The MC64 code is sequential, so when using this option in parallel, the graph is first gathered to the root process. The Combinatorial BLAS code can currently only be used in parallel, and only with a square number of processes.

*/

/*! \page Nested_Dissection_Recording Nested Dissection Recording

The STRUMPACK sparse solver supports both (Par)Metis and (PT-)Scotch for the matrix reordering. The following functions can set the preferred method or check the currently selected method:

\code {.cpp}
void SPOptions::set_reordering_method(ReorderingStrategy m);    
ReorderingStrategy SPOptions::reordering_method() const;
\endcode

The options for MatrixReorderingStrategy are
    
\code {.cpp}
enum class ReorderingStrategy {    
    NATURAL,    /*!< Do not reorder the system */     
    METIS,      /*!< Use Metis nested-dissection reordering */     
    PARMETIS,   /*!< Use ParMetis nested-dissection reordering */     
    SCOTCH,     /*!< Use Scotch nested-dissection reordering */     
    PTSCOTCH,            
    RCM,        /*!< Use RCM reordering */     
    GEOMETRIC   /*!< A simple geometric nested dissection code that only works for regular meshes. (see Sp::reorder)  */    
};
\endcode

When the solver is an object of Sp, PARMETIS or PTSCOTCH are not supported. When the solver is parallel, either an SpMPI or SpMPIDist object, and METIS, SCOTCH or RCM are chosen, then the graph of the complete matrix will be gathered onto the root process and the root process will call the (sequential) Metis, Scotch or RCM reordering routine. For large graphs this might fail due to insufficient memory.

The GEOMETRIC option is only allowed for regular grids. In this case, the dimensions of the grid should be specified in the function

\code {.cpp} ReturnCode Sp::reorder(int nx=1, int ny=1, int nz=1); \endcode

For instance for a regular 2d 2000 \f$×\f$ 4000 grid, you can call this as sp.reorder(2000, 4000). In the general algebraic case, the grid dimensions don’t have to be provided. The reordering method can also be specified via the command line option

\code {.cpp} --sp_reordering_method [metis|parmetis|scotch|ptscotch|geometric|rcm] \endcode

*/

/*! \page factorization Factorization

Compute the factorization by calling
\code {.cpp} ReturnCode Sp::factor(); \endcode
where the possible return values are the same as for Sp::reorder(). If Sp::reorder() was not called already, it is called automatically. When HSS compression is not enabled, this will compute an exact LU factorization of the (permuted) sparse input matrix. If HSS compression is enabled (with SPOptions::enable_HSS() or \--sp_enable_HSS, see Section 5), the factorization is only approximate.

*/

/*! \page solve Solve

Solve the linear system \f$Ax = b\f$ by calling

\code {.cpp} 
ReturnCode strumpack::StrumpackSparseSolver::solve(scalar* b, scalar* x, bool use_initial_guess=false); 
\endcode

By default (bool use_initial_guess=false) the input in x is ignored. If bool use_initial_guess=true, x is used as initial guess for the iterative solver (if an iterative solver is used, for instance iterative refinement or GMRES). If the Sp::factor() was not called, it is called automatically. The return values are the same as for Sp::reorder().

The iterative solver can be chosen through:

\code {.cpp} void strumpack::SPOptions::set_Krylov_solver(strumpack::KrylovSolver s);\endcode

where \code {.cpp} KrylovSolver \endcode can take the following values:

\code {.cpp}
enum class KrylovSolver {    
    AUTO,              /*!< Use iterative refinement if no HSS compression is used, otherwise > PGMRES. */      
    DIRECT,            /*!< No outer iterative solver, just a single application of the  multifrontal solver. */     
    REFINE,            /*!< Iterative refinement. */     
    PREC_GMRES,        /*!< Preconditioned GMRES. The preconditioner is the (approx)  multifrontal solver. */     
    GMRES,             /*!< UN-preconditioned GMRES. (for testing mainly) */     
    PREC_BICGSTAB,     /*!< Preconditioned BiCGStab. The preconditioner is the (approx) > multifrontal solver. */     
    BICGSTAB           /*!< UN-preconditioned BiCGStab. (for testing mainly) */    
};
\endcode

with KrylovSolver::AUTO being the default value. The KrylovSolver::AUTO setting will use iterative refinement when HSS compression is not enabled, and preconditioned GMRES when HSS compression is enabled, see Section 5. To use the solver as a preconditioner, or a single (approximate) solve, set the solver to KrylovSolver::DIRECT. When calling SpMPIDist::solve, the right-hand side and solution vectors should only point to the local parts!

*/

/*! \page sparse_solver_options All Options for the Sparse Solver

The HSS specific options are stored in an object of type HSSOptions<scalar>, inside the SPOptions object. These options are described in Section 5.

____

- \subpage scalar_interface
- \subpage command_options

*/

/*! \page scalar_interface SPOptions<scalar> Interface

The complete public interface to the options class is as follows, wher the real type is the real part of a scalar,
i.e., decltype(std::real(scalar(0))).

StrumpackOptions.hpp

\code {.cpp}
template<typename scalar> class SPOptions {    
public:    
    SPOptions();    
    SPOptions(int argc, char* argv[]);    
   
    /* print statistics? */    
    void set_verbose(bool verbose);                     bool verbose() const;    
    /* maximum iterations in iterative solver */    
    void set_maxit(int maxit);                          int maxit() const;    
    /* relative residual stopping criterion for iterative solver */    
    void set_rel_tol(real rel_tol);                     real rel_tol() const;     
    /* absolute residual stopping criterion for iterative solver */    
    void set_abs_tol(real abs_tol);                     real abs_tol() const;    
    /* type of iterative solver to use, see section 4.9 */    
    void set_Krylov_solver(KrylovSolver s);             KrylovSolver Krylov_solver() const;    
    /* GMRES restart */    
    void set_gmres_restart(int m);                      int gmres_restart() const;    
    /* type of Gram-Schmidt used in GMRES */    
    void set_GramSchmidt_type(GramSchmidtType t);       GramSchmidtType GramSchmidt_type() const;    
    /* nested-dissection code, see section 4.7.2 */    
    void set_reordering_method(ReorderingStrategy m);   ReorderingStrategy reordering_method() const;    
    /* stop nested-dissection when domains are smaller than nd_param */    
    void set_nd_param(int nd_param);                    int nd_param() const;    
    /* use the internal (undocumented) metis routine METIS_NodeNDP */    
    void enable_METIS_NodeNDP();                        bool use_METIS_NodeNDP() const;     
    /* do not use METIS_NodeNDP, use METIS_NodeND instead */    
    void disable_METIS_NodeNDP();    
    /* use METIS_NodeND instead of METIS_NodeDNP */    
    void enable_METIS_NodeND();                         bool use_METIS_NodeND() const;    
    /* do not use METIS_NodeND, use METIS_NodeNDP instead */    
    void disable_METIS_NodeND();    
    /* build the supernodal tree using the MUMPS_SYMQAMD code */    
    void enable_MUMPS_SYMQAMD();                        bool use_MUMPS_SYMQAMD() const;    
    /* do not use MUMPS_SYMQAMD, use fundamental supernodes */    
    void disable_MUMPS_SYMQAMD();    
    /* when using MUMPS_SYMQAMS, enable aggressive amalgamation */    
    void enable_agg_amalg();                            bool use_agg_amalg() const;     
    /* when using MUMPS_SYMQAMS, disable aggressive amalgamation */    
    void disable_agg_amalg();    
    /* set the job to be used for static pivoting, see section 4.7.1 */    
    void set_matching(MatchingJob job);                 MatchingJob matching() const;    
    /* not used at the moment */    
    void enable_assembly_tree_log();                    bool log_assembly_tree() const;    
    void disable_assembly_tree_log();
  
    /* enable HSS compression, see section 5 */     
    void enable_HSS();                                  bool use_HSS() const;    
    /* disable HSS compression */    
    void disable_HSS();    
    /* set the minimum size of a separator for HSS compression */    
    void set_HSS_min_sep_size(int s);                   int HSS_min_sep_size() const;

    /* set level to 1 to enable length 2 connections in the separator  
    * before computing separator reordering to reduce HSS ranks.     
    * Set to to disable length 2 connections. */    
    void set_separator_ordering_level(int l);           int separator_ordering_level() const;
      
    /* best not to touch this */     
    void enable_indirect_sampling();    
    void disable_indirect_sampling();                   bool indirect_sampling() const;
 
    void enable_replace_tiny_pivots();                  bool replace_tiny_pivots() const;    
    void disable_replace_tiny_pivots(;
       
    /* get the HSS specific options, see section 5. */    
    const HSS::HSSOptions<scalar>& HSS_options() const;    
    HSS::HSSOptions<scalar>& HSS_options();
        
    /* parse the options in argc/argv set in the constructor */    
    void set_from_command_line();    
    /* parse the options in argc/argv */     
    void set_from_command_line(int argc, char* argv[]);    
    /* print out message listing all command line options */     
    void describe_options() const;    
};
\endcode

This uses the following (scoped) enumeration for the Gram-Schmidt type used in GMRES:

\code {.cpp}
enum class GramSchmidtType {    
    CLASSICAL,  /*!< Classical Gram-Schmidt is faster, more scalable. */    
    MODIFIED    /*!< Modified Gram-Schmidt is slower, but stable. */    
};
\endcode

<img src="/Users/lucyguo/Desktop/LBL1/STRUMPACK/doc/doxygen/images/SPOptions_Interface.png" width = 25%>

Figure 3: Illustration of a Hierarchically Semi-Separable (HSS) matrix. Gray blocks are dense matrices. Off- diagonal blocks, on different levels of the HSS hierarchy, are low-rank. The low-rank factors of off-diagonal blocks of different levels are related.

*/

/*! \page command_options Command Line Options

To get a list of all available options, make sure to pass “int argc, char* argv[]” when initializing the StrumpackSparseSolver or when calling SPOptions::set_from_command_line and run the application with --help or -h. Some default values listed here are for double precision and might be different when running in single precision.       

STRUMPACK options:
\code {.bash}
--sp_maxit int (default 5000)    
--sp_rel_tol real (default 1e-06)    
--sp_abs_tol real (default 1e-10)    
--sp_Krylov_solver auto|direct|refinement|pgmres|gmres|pbicgstab|bicgstab --sp_gmres_restart int (default 30)    
--sp_GramSchmidt_type modified|classical    
--sp_reordering_method natural|metis|scotch|parmetis|ptscotch|rcm|geometric --sp_nd_param int (default 8)    
--sp_enable_METIS_NodeNDP (default true)    
--sp_disable_METIS_NodeNDP (default false)    
--sp_enable_METIS_NodeND (default false)    
--sp_disable_METIS_NodeND (default true)    
--sp_enable_MUMPS_SYMQAMD (default false)    
--sp_disable_MUMPS_SYMQAMD (default true)    
--sp_enable_agg_amalg (default false)    
--sp_disable_agg_amalg (default true)    
--sp_matching 0-6 (default 5)    
--sp_enable_hss (default false)    
--sp_disable_hss (default true)    
--sp_hss_min_sep_size int (default 256)    
--sp_separator_ordering_level (default 1)    
--sp_enable_indirect_sampling    
--sp_disable_indirect_sampling    
--sp_enable_replace_tiny_pivots    
--sp_disable_replace_tiny_pivots    
--sp_verbose or -v (default true)    
--sp_quiet or -q (default false)    
--help or -h
\endcode

*/
*/


